#!/bin/bash

# This script does some very basic benchmarks with 'qsv' using a 1M row 
# city population data set, and a 100k row sample of NYC's 311 data. If these
# doesn't exist on your system, it will be downloaded for you.
#
# These aren't meant to be overly rigorous, but they should be enough to catch
# significant regressions.
#
# Make sure you're using a release-optimized `qsv - generated by 
# `cargo build --release`; `cargo install qsv`; or `cargo install --path .` 
# issued from the root of your qsv git repo.
#
# This shell script has been tested on Linux, macOS and Cygwin for Windows.
# On Cygwin, make sure to install `unzip`, `bc` and `time`.

set -e

pat="$1"
# you can change bin_name to another binary, like xsv
bin_name=qsv
datazip=worldcitiespop_mil.zip
data=worldcitiespop_mil.csv
countrydata=countrynames.csv
data_idx=worldcitiespop_mil.csv.idx
data_to_exclude=data_to_exclude.csv
searchset_patterns=searchset_patterns.txt
nyc311datazip=nyc-311-sample-100k.zip
nyc311data=nyc-311-sample-100k.csv
if [ ! -r "$data" ]; then
  printf "Downloading benchmarking data...\n"
  curl -sS https://raw.githubusercontent.com/wiki/jqnatividad/qsv/files/worldcitiespop_mil.zip > "$datazip"
  unzip "$datazip"
  "$bin_name" sample --seed 42 50000 "$data" -o "$data_to_exclude"
  printf "santa\nfort\ncamp\n" > "$searchset_patterns"
  curl -sS https://raw.githubusercontent.com/wiki/jqnatividad/qsv/files/nyc-311-sample-100k.zip > "$nyc311datazip"
  unzip "$nyc311datazip"
fi
os_type=$(echo $OSTYPE | cut -c 1-6)
if [[ "$os_type" == "darwin" ]]; then
  data_size=$(stat -f '%z' "$data")
  nyc311data_size=$(stat -f '%z' "$nyc311data")
else
  data_size=$(stat --format '%s' "$data")
  nyc311data_size=$(stat --format '%s' "$nyc311data")
fi
if [ ! -r "$countydata" ]; then
  curl -sS https://gist.githubusercontent.com/anonymous/063cb470e56e64e98cf1/raw/98e2589b801f6ca3ff900b01a87fbb7452eb35c7/countrynames.csv > "$countrydata"
fi

function real_seconds {
  cmd=$(echo $@ "> /dev/null 2>&1")
  t=$(
    $(which time) -p sh -c "$cmd" 2>&1 \
      | grep '^real' \
      | awk '{print $2}')
  if [ $(echo "$t < 0.01" | bc) = 1 ]; then
    t=0.01
  fi
  echo $t
}

function benchmark {
  rm -f "$data_idx"
  t1=$(real_seconds "$@")
  rm -f "$data_idx"
  t2=$(real_seconds "$@")
  rm -f "$data_idx"
  t3=$(real_seconds "$@")
  echo "scale=2; ($t1 + $t2 + $t3) / 3" | bc
}

function benchmark_with_index {
  rm -f "$data_idx"
  "$bin_name" index "$data"
  t1=$(real_seconds "$@")
  t2=$(real_seconds "$@")
  t3=$(real_seconds "$@")
  rm -f "$data_idx"
  echo "scale=2; ($t1 + $t2 + $t3) / 3" | bc
}

function run {
  index=
  nyc311=
  while true; do
    case "$1" in
      --index) index="yes" && shift ;;
      --nyc311) nyc311="yes" && shift ;;
      *) break ;;
    esac
  done
  name="$1"
  shift

  printf "%-27s" "$name"
  if [ -z "$pat" ] || echo "$name" | grep -E -q "^$pat$"; then
    if [ -z "$index" ]; then
      t=$(benchmark "$@")
    else
      t=$(benchmark_with_index "$@")
    fi
    if [ -z "$nyc311" ]; then
      mb_per=$(echo "scale=2; ($data_size / $t) / 2^20" | bc)
      recs_per=$(echo "scale=2; (1000000 / $t)" | bc)
    else
      mb_per=$(echo "scale=2; ($nyc311data_size / $t) / 2^20" | bc)
      recs_per=$(echo "scale=2; (100000 / $t)" | bc)
    fi
    mb_per=$(printf "%0.02f" $mb_per)
    recs_per=$(printf "%'.2f" $recs_per)
    printf -v tprint "%0.02f" $t
    printf "%-11s%-12s%-12s\n" "$tprint" "$mb_per" "$recs_per"
    printf "%s\t%0.02f\t%s\t%s\n" $name $t $mb_per $recs_per >> $benchmarkfile
  fi
}

binver=$("$bin_name" --version)
current_time=$(date "+%Y-%m-%d-%H-%M-%S")
# printf "Scrambling benchmark data...\n"
# "$bin_name" index "$data"
# "$bin_name" scramble "$data" > temp.csv
# mv -f temp.csv "$data"
benchmarkfile=$bin_name-bench-$binver-$current_time.tsv
printf "%-27s%-11s%-12s%-12s\n" BENCHMARK TIME_SECS MB_PER_SEC RECS_PER_SEC
printf "benchmark\ttime_secs\tmb_per_sec\trecs_per_sec\n" > $benchmarkfile
run apply_op_string "$bin_name" apply operations lower Country "$data"
run apply_op_string_ascii "$bin_name" apply operations asciilower Country "$data"
run apply_op_similarity "$bin_name" apply operations lower,simdln Country --comparand union --new-column Country_sim-union_score "$data"
run apply_op_soundex "$bin_name" apply operations lower,soundex City --comparand boston --new-column City_boston_soundex "$data" 
run --nyc311 apply_datefmt "$bin_name" apply datefmt \"Created Date\" "$nyc311data"
run --nyc311 apply_emptyreplace "$bin_name" apply emptyreplace \"Bridge Highway Name\" --replacement Unspecified "$nyc311data"
run --nyc311 apply_geocode "$bin_name" apply geocode Location --new-column geocoded_location -q "$nyc311data"
run count "$bin_name" count "$data"
run --index count_index "$bin_name" count "$data"
run dedup "$bin_name" dedup "$data"
run enum "$bin_name" enum "$data"
run exclude "$bin_name" exclude Country "$data" Country "$data_to_exclude"
run --index exclude_index "$bin_name" exclude Country "$data" Country "$data_to_exclude"
run explode "$bin_name" explode City "-" "$data"
run fill "$bin_name" fill -v Unknown Population "$data"
run fixlengths "$bin_name" fixlengths "$data"
run flatten "$bin_name" flatten "$data"
run flatten_condensed "$bin_name" flatten "$data" --condense 50
run fmt "$bin_name" fmt --crlf "$data"
run frequency "$bin_name" frequency "$data"
run --index frequency_index "$bin_name" frequency "$data"
run frequency_selregex "$bin_name" frequency -s /^R/ "$data"
run frequency_j1 "$bin_name" frequency -j 1 "$data"
run index "$bin_name" index "$data"
run join "$bin_name" join --no-case Country "$data" Abbrev "$countrydata"
run lua "$bin_name" lua map pop_empty "tonumber\(Population\)==nil" "$data"
run partition "$bin_name" partition Region /tmp/partitioned "$data"
run rename "$bin_name" rename 'country,city,accent_city,region,population,lat,long' "$data"
run reverse "$bin_name" reverse "$data"
run sample_10 "$bin_name" sample 10 "$data"
run --index sample_10_index "$bin_name" sample 10 "$data"
run sample_1000 "$bin_name" sample 1000 "$data"
run --index sample_1000_index "$bin_name" sample 1000 "$data"
run sample_100000 "$bin_name" sample 100000 "$data"
run --index sample_100000_index "$bin_name" sample 100000 "$data"
run --index sample_25pct_index "$bin_name" sample 0.25 "$data"
run --index scramble_index "$bin_name" scramble "$data"
run search "$bin_name" search -s Country "'(?i)us'" "$data"
run searchset "$bin_name" searchset "$searchset_patterns" "$data"
run select "$bin_name" select 'Country,City' "$data"
run select_regex "$bin_name" select /^L/ "$data"
run slice_one_middle "$bin_name" slice -i 500000 "$data"
run --index slice_one_middle_index "$bin_name" slice -i 500000 "$data"
run sort "$bin_name" sort -s Country "$data"
run split "$bin_name" split --size 50000 split_tempdir "$data"
run --index split_index "$bin_name" split --size 50000 split_tempdir "$data"
run --index split_index_j1 "$bin_name" split --size 50000 -j 1 split_tempdir "$data"
run stats "$bin_name" stats "$data"
run --index stats_index "$bin_name" stats "$data"
run --index stats_index_j1 "$bin_name" stats -j 1 "$data"
run stats_everything "$bin_name" stats "$data" --everything
run stats_everything_j1 "$bin_name" stats "$data" --everything -j 1
run --index stats_everything_index "$bin_name" stats "$data" --everything
run --index stats_everything_index_j1 "$bin_name" stats "$data" --everything -j 1
run table "$bin_name" table "$data"
run transpose "$bin_name" transpose "$data"
